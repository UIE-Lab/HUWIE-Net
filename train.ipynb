{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/UIE-Lab/HUWIE-Net/blob/main/train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ek6OyRc6sexa"
      },
      "source": [
        "\n",
        "# **Training HUWIE-Net**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wB6uavxPmiMh"
      },
      "source": [
        "**Cloning the HUWIE-Net Repository**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N1Zb9cxYJl-a",
        "outputId": "39ffbfa1-5039-41a0-ffa2-cae85e4e8275"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into '/content/Experiments/HUWIE-Net'...\n",
            "remote: Enumerating objects: 261, done.\u001b[K\n",
            "remote: Counting objects: 100% (51/51), done.\u001b[K\n",
            "remote: Compressing objects: 100% (50/50), done.\u001b[K\n",
            "remote: Total 261 (delta 22), reused 0 (delta 0), pack-reused 210 (from 1)\u001b[K\n",
            "Receiving objects: 100% (261/261), 6.90 MiB | 12.58 MiB/s, done.\n",
            "Resolving deltas: 100% (118/118), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/UIE-Lab/HUWIE-Net.git /content/Experiments/HUWIE-Net"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JfM7EPkzm1g3"
      },
      "source": [
        "**Downloading and Extracting the Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nTaYUjlPKEZq"
      },
      "outputs": [],
      "source": [
        "!pip install gdown\n",
        "!gdown --id 1ft9mlhO5JodH3Aw9Ak_l3tMz8pjNec9p\n",
        "!unzip Data.zip -d ./\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mi4wCqoEnZFl"
      },
      "source": [
        "**Importing Modules**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U42sF-SVKtXg"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.chdir('/content/Experiments/HUWIE-Net')\n",
        "\n",
        "import sys\n",
        "import time\n",
        "import argparse\n",
        "from getpass import getuser\n",
        "from socket import gethostname\n",
        "import logging\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from core.models import HUWIE_Net, HUWIE_Net_I2IM, HUWIE_Net_PIM\n",
        "from core.losses import Loss\n",
        "from core.datasets import UIEBD\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pjfbqr1hnrSa"
      },
      "source": [
        "**Configuration and Setup**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I-X9QG7VLbWT"
      },
      "outputs": [],
      "source": [
        "parser = argparse.ArgumentParser(description='OD')\n",
        "parser.add_argument('--name', type=str, default='Train145')\n",
        "parser.add_argument('--work_dir', type=str, default='../../Data/checkpoints/')\n",
        "parser.add_argument('--UIEBD_konf', type=int, default=3)\n",
        "parser.add_argument('--model', type=object, default=HUWIE_Net)\n",
        "parser.add_argument('--loss', type=object, default=Loss)\n",
        "parser.add_argument('--epochs', type=int, default=1)\n",
        "parser.add_argument('--lr', type=float, default=1e-3)\n",
        "parser.add_argument('--train_val_batch_size', type=int, default=8)\n",
        "parser.add_argument('--test_batch_size', type=int, default=1)\n",
        "parser.add_argument('--step_size', type=int, default=50)\n",
        "parser.add_argument('--gamma', type=float, default=0.5)\n",
        "args, unknown = parser.parse_known_args()\n",
        "print(args)\n",
        "\n",
        "timestamp = time.strftime('%Y%m%d_%H%M%S', time.localtime())\n",
        "work_dir = args.work_dir + args.name + '_' + args.model.__name__ + '_' + timestamp + '/'\n",
        "p = os.path.abspath(work_dir)\n",
        "if not os.path.exists(p): os.makedirs(p)\n",
        "work_dir_img_output = work_dir + 'img_output' '/'\n",
        "p = os.path.abspath(work_dir_img_output)\n",
        "if not os.path.exists(work_dir_img_output): os.makedirs(work_dir_img_output)\n",
        "\n",
        "# create text log\n",
        "logger = logging.getLogger(args.name)\n",
        "log_file = os.path.join(work_dir, f'{timestamp}.log')\n",
        "formatter = logging.Formatter('%(asctime)s - %(message)s')\n",
        "stream_handler = logging.StreamHandler()\n",
        "stream_handler.setFormatter(formatter)\n",
        "logger.addHandler(stream_handler)\n",
        "file_handler = logging.FileHandler(log_file, 'w', encoding='utf-8')\n",
        "file_handler.setFormatter(formatter)\n",
        "logger.addHandler(file_handler)\n",
        "logger.setLevel(logging.INFO)\n",
        "logger.info(args)\n",
        "\n",
        "# tensorboard log\n",
        "writer = SummaryWriter(os.path.join(work_dir, 'tensorboard_logs'))\n",
        "\n",
        "# dataset\n",
        "konf = args.UIEBD_konf\n",
        "train_dataset = UIEBD(data_type='train', konf=konf)\n",
        "val_dataset = UIEBD(data_type='val', konf=konf)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=args.train_val_batch_size, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=args.train_val_batch_size, shuffle=True)\n",
        "logger.info('Dataset: ' + train_dataset.__class__.__name__)\n",
        "\n",
        "# model\n",
        "model = args.model()\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "logger.info('Device: ' + str(device))\n",
        "model.to(device)\n",
        "\n",
        "# print model's state_dict\n",
        "logger.info(\"Model's state_dict:\")\n",
        "for param_tensor in model.state_dict():\n",
        "    logger.info(param_tensor + \" - \" + str(model.state_dict()[param_tensor].size()))\n",
        "logger.info('Finish Build Model')\n",
        "\n",
        "# optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=args.step_size, gamma=args.gamma)\n",
        "\n",
        "# print optimizer's state_dict\n",
        "logger.info(\"Optimizer's state_dict:\")\n",
        "for var_name in optimizer.state_dict():\n",
        "    logger.info(var_name + \" - \" + str(optimizer.state_dict()[var_name]))\n",
        "logger.info('Finish Build Optimizer')\n",
        "\n",
        "# loss\n",
        "criterion = args.loss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUOTipJ3nzTt"
      },
      "source": [
        "**Training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "V1kNLWwvM61U",
        "outputId": "f5e4c9df-de9b-4f9d-b640-7e76eba82501"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-12-11 17:25:33,347 - Host: root@9b5307146685, Work Dir: ../../Data/checkpoints/Train145_HUWIE_Net_20241211_172452/\n",
            "INFO:Train145:Host: root@9b5307146685, Work Dir: ../../Data/checkpoints/Train145_HUWIE_Net_20241211_172452/\n",
            "2024-12-11 17:25:33,354 - Epoch: 1, Train Iteration: 100, Validation Iteration: 12\n",
            "INFO:Train145:Epoch: 1, Train Iteration: 100, Validation Iteration: 12\n",
            "2024-12-11 17:25:33,359 - Start\n",
            "INFO:Train145:Start\n",
            "2024-12-11 17:25:33,374 - Epoch 1\n",
            "INFO:Train145:Epoch 1\n",
            "2024-12-11 17:27:22,818 - Train Epoch: [1][1/100] Time: 1.116 lr: 0.001000 l1_loss: 0.2014 (x1.0) ssim_loss: 0.5805 (x1.0) total_loss: 0.7819 (x1.0)\n",
            "INFO:Train145:Train Epoch: [1][1/100] Time: 1.116 lr: 0.001000 l1_loss: 0.2014 (x1.0) ssim_loss: 0.5805 (x1.0) total_loss: 0.7819 (x1.0)\n",
            "2024-12-11 17:29:05,814 - Train Epoch: [1][2/100] Time: 110.421 lr: 0.001000 l1_loss: 0.2172 (x1.0) ssim_loss: 0.3746 (x1.0) total_loss: 0.5918 (x1.0)\n",
            "INFO:Train145:Train Epoch: [1][2/100] Time: 110.421 lr: 0.001000 l1_loss: 0.2172 (x1.0) ssim_loss: 0.3746 (x1.0) total_loss: 0.5918 (x1.0)\n",
            "2024-12-11 17:30:52,791 - Train Epoch: [1][3/100] Time: 213.113 lr: 0.001000 l1_loss: 0.169 (x1.0) ssim_loss: 0.3025 (x1.0) total_loss: 0.4715 (x1.0)\n",
            "INFO:Train145:Train Epoch: [1][3/100] Time: 213.113 lr: 0.001000 l1_loss: 0.169 (x1.0) ssim_loss: 0.3025 (x1.0) total_loss: 0.4715 (x1.0)\n",
            "2024-12-11 17:32:41,748 - Train Epoch: [1][4/100] Time: 320.643 lr: 0.001000 l1_loss: 0.1684 (x1.0) ssim_loss: 0.2506 (x1.0) total_loss: 0.419 (x1.0)\n",
            "INFO:Train145:Train Epoch: [1][4/100] Time: 320.643 lr: 0.001000 l1_loss: 0.1684 (x1.0) ssim_loss: 0.2506 (x1.0) total_loss: 0.419 (x1.0)\n",
            "2024-12-11 17:34:24,656 - Train Epoch: [1][5/100] Time: 429.353 lr: 0.001000 l1_loss: 0.1342 (x1.0) ssim_loss: 0.2268 (x1.0) total_loss: 0.361 (x1.0)\n",
            "INFO:Train145:Train Epoch: [1][5/100] Time: 429.353 lr: 0.001000 l1_loss: 0.1342 (x1.0) ssim_loss: 0.2268 (x1.0) total_loss: 0.361 (x1.0)\n",
            "2024-12-11 17:36:09,373 - Train Epoch: [1][6/100] Time: 532.209 lr: 0.001000 l1_loss: 0.1449 (x1.0) ssim_loss: 0.2056 (x1.0) total_loss: 0.3505 (x1.0)\n",
            "INFO:Train145:Train Epoch: [1][6/100] Time: 532.209 lr: 0.001000 l1_loss: 0.1449 (x1.0) ssim_loss: 0.2056 (x1.0) total_loss: 0.3505 (x1.0)\n",
            "2024-12-11 17:37:52,846 - Train Epoch: [1][7/100] Time: 636.844 lr: 0.001000 l1_loss: 0.1339 (x1.0) ssim_loss: 0.1991 (x1.0) total_loss: 0.333 (x1.0)\n",
            "INFO:Train145:Train Epoch: [1][7/100] Time: 636.844 lr: 0.001000 l1_loss: 0.1339 (x1.0) ssim_loss: 0.1991 (x1.0) total_loss: 0.333 (x1.0)\n",
            "2024-12-11 17:39:34,727 - Train Epoch: [1][8/100] Time: 740.358 lr: 0.001000 l1_loss: 0.1352 (x1.0) ssim_loss: 0.2198 (x1.0) total_loss: 0.3551 (x1.0)\n",
            "INFO:Train145:Train Epoch: [1][8/100] Time: 740.358 lr: 0.001000 l1_loss: 0.1352 (x1.0) ssim_loss: 0.2198 (x1.0) total_loss: 0.3551 (x1.0)\n",
            "2024-12-11 17:41:16,479 - Train Epoch: [1][9/100] Time: 841.975 lr: 0.001000 l1_loss: 0.1438 (x1.0) ssim_loss: 0.1969 (x1.0) total_loss: 0.3407 (x1.0)\n",
            "INFO:Train145:Train Epoch: [1][9/100] Time: 841.975 lr: 0.001000 l1_loss: 0.1438 (x1.0) ssim_loss: 0.1969 (x1.0) total_loss: 0.3407 (x1.0)\n",
            "2024-12-11 17:43:00,505 - Train Epoch: [1][10/100] Time: 944.167 lr: 0.001000 l1_loss: 0.1417 (x1.0) ssim_loss: 0.1844 (x1.0) total_loss: 0.3261 (x1.0)\n",
            "INFO:Train145:Train Epoch: [1][10/100] Time: 944.167 lr: 0.001000 l1_loss: 0.1417 (x1.0) ssim_loss: 0.1844 (x1.0) total_loss: 0.3261 (x1.0)\n",
            "2024-12-11 17:44:41,603 - Train Epoch: [1][11/100] Time: 1047.946 lr: 0.001000 l1_loss: 0.1088 (x1.0) ssim_loss: 0.1669 (x1.0) total_loss: 0.2757 (x1.0)\n",
            "INFO:Train145:Train Epoch: [1][11/100] Time: 1047.946 lr: 0.001000 l1_loss: 0.1088 (x1.0) ssim_loss: 0.1669 (x1.0) total_loss: 0.2757 (x1.0)\n",
            "2024-12-11 17:46:23,837 - Train Epoch: [1][12/100] Time: 1149.111 lr: 0.001000 l1_loss: 0.1047 (x1.0) ssim_loss: 0.1623 (x1.0) total_loss: 0.267 (x1.0)\n",
            "INFO:Train145:Train Epoch: [1][12/100] Time: 1149.111 lr: 0.001000 l1_loss: 0.1047 (x1.0) ssim_loss: 0.1623 (x1.0) total_loss: 0.267 (x1.0)\n",
            "2024-12-11 17:48:07,432 - Train Epoch: [1][13/100] Time: 1251.221 lr: 0.001000 l1_loss: 0.1021 (x1.0) ssim_loss: 0.176 (x1.0) total_loss: 0.2781 (x1.0)\n",
            "INFO:Train145:Train Epoch: [1][13/100] Time: 1251.221 lr: 0.001000 l1_loss: 0.1021 (x1.0) ssim_loss: 0.176 (x1.0) total_loss: 0.2781 (x1.0)\n",
            "2024-12-11 17:49:48,908 - Train Epoch: [1][14/100] Time: 1354.772 lr: 0.001000 l1_loss: 0.1076 (x1.0) ssim_loss: 0.1498 (x1.0) total_loss: 0.2574 (x1.0)\n",
            "INFO:Train145:Train Epoch: [1][14/100] Time: 1354.772 lr: 0.001000 l1_loss: 0.1076 (x1.0) ssim_loss: 0.1498 (x1.0) total_loss: 0.2574 (x1.0)\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-b92ab4fbddeb>\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mtotal_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    579\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             )\n\u001b[0;32m--> 581\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    582\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    826\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# start\n",
        "train_iter = args.epochs * len(train_dataloader)\n",
        "val_iter = args.epochs * len(val_dataloader)\n",
        "\n",
        "logger.info('Host: %s, Work Dir: %s', f'{getuser()}@{gethostname()}', work_dir)\n",
        "logger.info('Epoch: %d, Train Iteration: %d, Validation Iteration: %d', args.epochs, train_iter, val_iter)\n",
        "\n",
        "logger.info('Start')\n",
        "t = time.time()\n",
        "\n",
        "# Çalışan kodda yüklenen tüm kütüphaneleri al\n",
        "loaded_modules = list(sys.modules.keys())\n",
        "with open(\"requirements.txt\", \"w\") as file:\n",
        "    for module in loaded_modules:\n",
        "        file.write(module + \"\\n\")\n",
        "\n",
        "for epoch in range(1, args.epochs + 1):\n",
        "\n",
        "    logger.info('Epoch %d', epoch)\n",
        "\n",
        "    ###---TRAIN---###\n",
        "\n",
        "    train_epoch_loss_item = np.zeros(criterion.loss_fn_num, dtype='float32')\n",
        "\n",
        "    model.train()\n",
        "    for i, tdata in enumerate(train_dataloader):\n",
        "\n",
        "        data_time = time.time() - t\n",
        "\n",
        "        inputs = tdata['raw_data'].to(device)\n",
        "        labels = tdata['gt_data'].to(device)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        losses, weight = criterion(outputs, labels)\n",
        "        total_loss = losses[-1]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        total_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        logger.info('Train Epoch: [%d][%d/%d] Time: %.3f lr: %f ' +\n",
        "                    ' '.join([criterion.loss_name[k] + ': ' + str(np.round(losses[k].item(), 4)) + ' (x' + str(weight[k]) + ')' for k in range(len(losses))]),\n",
        "                    epoch, i+1, len(train_dataloader), data_time, optimizer.param_groups[0]['lr'])\n",
        "\n",
        "        for k in range(len(losses)):\n",
        "            train_epoch_loss_item[k] += losses[k].item()\n",
        "\n",
        "        for loss in losses:\n",
        "            if torch.isnan(loss):\n",
        "                torch.save(model.state_dict(), work_dir + 'nan_loss_' + 'epoch{}.pth'.format(epoch))\n",
        "                logger.info('NaN loss...')\n",
        "                sys.exit()\n",
        "\n",
        "    train_epoch_loss_item = train_epoch_loss_item / len(train_dataloader)\n",
        "\n",
        "    logger.info('Train Epoch (Average): [%d] ' + ' '.join([criterion.loss_name[k] + ': ' + str(np.round(train_epoch_loss_item[k], 4)) for k in range(len(losses))]), epoch)\n",
        "\n",
        "    for k in range(len(train_epoch_loss_item)):\n",
        "        writer.add_scalars('Train vs. Val Loss', {'Train_' + criterion.loss_name[k]: train_epoch_loss_item[k]}, epoch)\n",
        "\n",
        "    for name, param in model.named_parameters():\n",
        "        writer.add_histogram('model_param/' + name, param, epoch)\n",
        "        writer.add_histogram('model_param_grad/' + name, param.grad, epoch)\n",
        "        writer.add_scalar('model_param_grad_abs_sum/' + name, torch.sum(torch.abs(param.grad)), epoch)\n",
        "\n",
        "    if epoch == args.epochs:\n",
        "        torch.save(model.state_dict(), work_dir + args.model.__name__ + '_epoch{}.pth'.format(epoch))\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    ###---VALIDATION---###\n",
        "\n",
        "    val_epoch_loss_item = np.zeros(criterion.loss_fn_num, dtype='float32')\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for j, vdata in enumerate(val_dataloader):\n",
        "\n",
        "            data_time = time.time() - t\n",
        "\n",
        "            vinputs = tdata['raw_data'].to(device)\n",
        "            vlabels = tdata['gt_data'].to(device)\n",
        "\n",
        "            voutputs  = model(vinputs)\n",
        "\n",
        "            losses, weight = criterion(voutputs, vlabels)\n",
        "            total_loss = losses[-1]\n",
        "\n",
        "            logger.info('Val Epoch: [%d][%d/%d] Time: %.3f ' +\n",
        "                        ' '.join([criterion.loss_name[k] + ': ' + str(np.round(losses[k].item(), 4)) + ' (x' + str(weight[k]) + ')' for k in range(len(losses))]),\n",
        "                        epoch, i+1, len(val_dataloader), data_time)\n",
        "\n",
        "            for k in range(len(losses)):\n",
        "                val_epoch_loss_item[k] += losses[k].item()\n",
        "\n",
        "    val_epoch_loss_item = val_epoch_loss_item / len(val_dataloader)\n",
        "\n",
        "    logger.info('Val Epoch (Average): [%d] ' + ' '.join([criterion.loss_name[k] + ': ' + str(np.round(val_epoch_loss_item[k], 4)) for k in range(len(losses))]), epoch)\n",
        "\n",
        "    for k in range(len(train_epoch_loss_item)):\n",
        "        writer.add_scalars('Train vs. Val Loss', {'Val_' + criterion.loss_name[k]: val_epoch_loss_item[k]}, epoch)\n",
        "\n",
        "writer.flush()\n",
        "writer.close()\n",
        "\n",
        "logger.info('Finish')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}