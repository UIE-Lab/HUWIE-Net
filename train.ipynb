{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["\n","# **Training HUWIE-Net**"],"metadata":{"id":"ek6OyRc6sexa"}},{"cell_type":"markdown","source":["**Cloning the HUWIE-Net Repository**"],"metadata":{"id":"wB6uavxPmiMh"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"N1Zb9cxYJl-a","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733937000448,"user_tz":-180,"elapsed":1776,"user":{"displayName":"ozan demir","userId":"17678299881814955691"}},"outputId":"39ffbfa1-5039-41a0-ffa2-cae85e4e8275"},"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into '/content/Experiments/HUWIE-Net'...\n","remote: Enumerating objects: 261, done.\u001b[K\n","remote: Counting objects: 100% (51/51), done.\u001b[K\n","remote: Compressing objects: 100% (50/50), done.\u001b[K\n","remote: Total 261 (delta 22), reused 0 (delta 0), pack-reused 210 (from 1)\u001b[K\n","Receiving objects: 100% (261/261), 6.90 MiB | 12.58 MiB/s, done.\n","Resolving deltas: 100% (118/118), done.\n"]}],"source":["!git clone https://github.com/UIE-Lab/HUWIE-Net.git /content/Experiments/HUWIE-Net"]},{"cell_type":"markdown","source":["**Downloading and Extracting the Dataset**"],"metadata":{"id":"JfM7EPkzm1g3"}},{"cell_type":"code","source":["!pip install gdown\n","!gdown --id 1ft9mlhO5JodH3Aw9Ak_l3tMz8pjNec9p\n","!unzip Data.zip -d ./\n"],"metadata":{"id":"nTaYUjlPKEZq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Importing Modules**"],"metadata":{"id":"Mi4wCqoEnZFl"}},{"cell_type":"code","source":["import os\n","os.chdir('/content/Experiments/HUWIE-Net')\n","\n","import sys\n","import time\n","import argparse\n","from getpass import getuser\n","from socket import gethostname\n","import logging\n","import numpy as np\n","import torch\n","from torch.utils.data import DataLoader\n","from torch.utils.tensorboard import SummaryWriter\n","from core.models import HUWIE_Net, HUWIE_Net_I2IM, HUWIE_Net_PIM\n","from core.losses import Loss\n","from core.datasets import UIEBD\n"],"metadata":{"id":"U42sF-SVKtXg","executionInfo":{"status":"ok","timestamp":1733937884925,"user_tz":-180,"elapsed":20295,"user":{"displayName":"ozan demir","userId":"17678299881814955691"}}},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":["**Configuration and Setup**"],"metadata":{"id":"Pjfbqr1hnrSa"}},{"cell_type":"code","source":["parser = argparse.ArgumentParser(description='OD')\n","parser.add_argument('--name', type=str, default='Train145')\n","parser.add_argument('--work_dir', type=str, default='../../Data/checkpoints/')\n","parser.add_argument('--UIEBD_konf', type=int, default=3)\n","parser.add_argument('--model', type=object, default=HUWIE_Net)\n","parser.add_argument('--loss', type=object, default=Loss)\n","parser.add_argument('--epochs', type=int, default=1)\n","parser.add_argument('--lr', type=float, default=1e-3)\n","parser.add_argument('--train_val_batch_size', type=int, default=8)\n","parser.add_argument('--test_batch_size', type=int, default=1)\n","parser.add_argument('--step_size', type=int, default=50)\n","parser.add_argument('--gamma', type=float, default=0.5)\n","args, unknown = parser.parse_known_args()\n","print(args)\n","\n","timestamp = time.strftime('%Y%m%d_%H%M%S', time.localtime())\n","work_dir = args.work_dir + args.name + '_' + args.model.__name__ + '_' + timestamp + '/'\n","p = os.path.abspath(work_dir)\n","if not os.path.exists(p): os.makedirs(p)\n","work_dir_img_output = work_dir + 'img_output' '/'\n","p = os.path.abspath(work_dir_img_output)\n","if not os.path.exists(work_dir_img_output): os.makedirs(work_dir_img_output)\n","\n","# create text log\n","logger = logging.getLogger(args.name)\n","log_file = os.path.join(work_dir, f'{timestamp}.log')\n","formatter = logging.Formatter('%(asctime)s - %(message)s')\n","stream_handler = logging.StreamHandler()\n","stream_handler.setFormatter(formatter)\n","logger.addHandler(stream_handler)\n","file_handler = logging.FileHandler(log_file, 'w', encoding='utf-8')\n","file_handler.setFormatter(formatter)\n","logger.addHandler(file_handler)\n","logger.setLevel(logging.INFO)\n","logger.info(args)\n","\n","# tensorboard log\n","writer = SummaryWriter(os.path.join(work_dir, 'tensorboard_logs'))\n","\n","# dataset\n","konf = args.UIEBD_konf\n","train_dataset = UIEBD(data_type='train', konf=konf)\n","val_dataset = UIEBD(data_type='val', konf=konf)\n","train_dataloader = DataLoader(train_dataset, batch_size=args.train_val_batch_size, shuffle=True)\n","val_dataloader = DataLoader(val_dataset, batch_size=args.train_val_batch_size, shuffle=True)\n","logger.info('Dataset: ' + train_dataset.__class__.__name__)\n","\n","# model\n","model = args.model()\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","logger.info('Device: ' + str(device))\n","model.to(device)\n","\n","# print model's state_dict\n","logger.info(\"Model's state_dict:\")\n","for param_tensor in model.state_dict():\n","    logger.info(param_tensor + \" - \" + str(model.state_dict()[param_tensor].size()))\n","logger.info('Finish Build Model')\n","\n","# optimizer\n","optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n","scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=args.step_size, gamma=args.gamma)\n","\n","# print optimizer's state_dict\n","logger.info(\"Optimizer's state_dict:\")\n","for var_name in optimizer.state_dict():\n","    logger.info(var_name + \" - \" + str(optimizer.state_dict()[var_name]))\n","logger.info('Finish Build Optimizer')\n","\n","# loss\n","criterion = args.loss()"],"metadata":{"id":"I-X9QG7VLbWT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Training**"],"metadata":{"id":"rUOTipJ3nzTt"}},{"cell_type":"code","source":["# start\n","train_iter = args.epochs * len(train_dataloader)\n","val_iter = args.epochs * len(val_dataloader)\n","\n","logger.info('Host: %s, Work Dir: %s', f'{getuser()}@{gethostname()}', work_dir)\n","logger.info('Epoch: %d, Train Iteration: %d, Validation Iteration: %d', args.epochs, train_iter, val_iter)\n","\n","logger.info('Start')\n","t = time.time()\n","\n","# Çalışan kodda yüklenen tüm kütüphaneleri al\n","loaded_modules = list(sys.modules.keys())\n","with open(\"requirements.txt\", \"w\") as file:\n","    for module in loaded_modules:\n","        file.write(module + \"\\n\")\n","\n","for epoch in range(1, args.epochs + 1):\n","\n","    logger.info('Epoch %d', epoch)\n","\n","    ###---TRAIN---###\n","\n","    train_epoch_loss_item = np.zeros(criterion.loss_fn_num, dtype='float32')\n","\n","    model.train()\n","    for i, tdata in enumerate(train_dataloader):\n","\n","        data_time = time.time() - t\n","\n","        inputs = tdata['raw_data'].to(device)\n","        labels = tdata['gt_data'].to(device)\n","\n","        outputs = model(inputs)\n","\n","        losses, weight = criterion(outputs, labels)\n","        total_loss = losses[-1]\n","\n","        optimizer.zero_grad()\n","        total_loss.backward()\n","        optimizer.step()\n","\n","        logger.info('Train Epoch: [%d][%d/%d] Time: %.3f lr: %f ' +\n","                    ' '.join([criterion.loss_name[k] + ': ' + str(np.round(losses[k].item(), 4)) + ' (x' + str(weight[k]) + ')' for k in range(len(losses))]),\n","                    epoch, i+1, len(train_dataloader), data_time, optimizer.param_groups[0]['lr'])\n","\n","        for k in range(len(losses)):\n","            train_epoch_loss_item[k] += losses[k].item()\n","\n","        for loss in losses:\n","            if torch.isnan(loss):\n","                torch.save(model.state_dict(), work_dir + 'nan_loss_' + 'epoch{}.pth'.format(epoch))\n","                logger.info('NaN loss...')\n","                sys.exit()\n","\n","    train_epoch_loss_item = train_epoch_loss_item / len(train_dataloader)\n","\n","    logger.info('Train Epoch (Average): [%d] ' + ' '.join([criterion.loss_name[k] + ': ' + str(np.round(train_epoch_loss_item[k], 4)) for k in range(len(losses))]), epoch)\n","\n","    for k in range(len(train_epoch_loss_item)):\n","        writer.add_scalars('Train vs. Val Loss', {'Train_' + criterion.loss_name[k]: train_epoch_loss_item[k]}, epoch)\n","\n","    for name, param in model.named_parameters():\n","        writer.add_histogram('model_param/' + name, param, epoch)\n","        writer.add_histogram('model_param_grad/' + name, param.grad, epoch)\n","        writer.add_scalar('model_param_grad_abs_sum/' + name, torch.sum(torch.abs(param.grad)), epoch)\n","\n","    if epoch == args.epochs:\n","        torch.save(model.state_dict(), work_dir + args.model.__name__ + '_epoch{}.pth'.format(epoch))\n","\n","    scheduler.step()\n","\n","    ###---VALIDATION---###\n","\n","    val_epoch_loss_item = np.zeros(criterion.loss_fn_num, dtype='float32')\n","\n","    model.eval()\n","    with torch.no_grad():\n","        for j, vdata in enumerate(val_dataloader):\n","\n","            data_time = time.time() - t\n","\n","            vinputs = tdata['raw_data'].to(device)\n","            vlabels = tdata['gt_data'].to(device)\n","\n","            voutputs  = model(vinputs)\n","\n","            losses, weight = criterion(voutputs, vlabels)\n","            total_loss = losses[-1]\n","\n","            logger.info('Val Epoch: [%d][%d/%d] Time: %.3f ' +\n","                        ' '.join([criterion.loss_name[k] + ': ' + str(np.round(losses[k].item(), 4)) + ' (x' + str(weight[k]) + ')' for k in range(len(losses))]),\n","                        epoch, i+1, len(val_dataloader), data_time)\n","\n","            for k in range(len(losses)):\n","                val_epoch_loss_item[k] += losses[k].item()\n","\n","    val_epoch_loss_item = val_epoch_loss_item / len(val_dataloader)\n","\n","    logger.info('Val Epoch (Average): [%d] ' + ' '.join([criterion.loss_name[k] + ': ' + str(np.round(val_epoch_loss_item[k], 4)) for k in range(len(losses))]), epoch)\n","\n","    for k in range(len(train_epoch_loss_item)):\n","        writer.add_scalars('Train vs. Val Loss', {'Val_' + criterion.loss_name[k]: val_epoch_loss_item[k]}, epoch)\n","\n","writer.flush()\n","writer.close()\n","\n","logger.info('Finish')"],"metadata":{"id":"V1kNLWwvM61U","colab":{"base_uri":"https://localhost:8080/"},"outputId":"13229cbb-9f92-439d-f15d-123dc45bbc22"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["2024-12-11 17:25:33,347 - Host: root@9b5307146685, Work Dir: ../../Data/checkpoints/Train145_HUWIE_Net_20241211_172452/\n","INFO:Train145:Host: root@9b5307146685, Work Dir: ../../Data/checkpoints/Train145_HUWIE_Net_20241211_172452/\n","2024-12-11 17:25:33,354 - Epoch: 1, Train Iteration: 100, Validation Iteration: 12\n","INFO:Train145:Epoch: 1, Train Iteration: 100, Validation Iteration: 12\n","2024-12-11 17:25:33,359 - Start\n","INFO:Train145:Start\n","2024-12-11 17:25:33,374 - Epoch 1\n","INFO:Train145:Epoch 1\n","2024-12-11 17:27:22,818 - Train Epoch: [1][1/100] Time: 1.116 lr: 0.001000 l1_loss: 0.2014 (x1.0) ssim_loss: 0.5805 (x1.0) total_loss: 0.7819 (x1.0)\n","INFO:Train145:Train Epoch: [1][1/100] Time: 1.116 lr: 0.001000 l1_loss: 0.2014 (x1.0) ssim_loss: 0.5805 (x1.0) total_loss: 0.7819 (x1.0)\n","2024-12-11 17:29:05,814 - Train Epoch: [1][2/100] Time: 110.421 lr: 0.001000 l1_loss: 0.2172 (x1.0) ssim_loss: 0.3746 (x1.0) total_loss: 0.5918 (x1.0)\n","INFO:Train145:Train Epoch: [1][2/100] Time: 110.421 lr: 0.001000 l1_loss: 0.2172 (x1.0) ssim_loss: 0.3746 (x1.0) total_loss: 0.5918 (x1.0)\n","2024-12-11 17:30:52,791 - Train Epoch: [1][3/100] Time: 213.113 lr: 0.001000 l1_loss: 0.169 (x1.0) ssim_loss: 0.3025 (x1.0) total_loss: 0.4715 (x1.0)\n","INFO:Train145:Train Epoch: [1][3/100] Time: 213.113 lr: 0.001000 l1_loss: 0.169 (x1.0) ssim_loss: 0.3025 (x1.0) total_loss: 0.4715 (x1.0)\n","2024-12-11 17:32:41,748 - Train Epoch: [1][4/100] Time: 320.643 lr: 0.001000 l1_loss: 0.1684 (x1.0) ssim_loss: 0.2506 (x1.0) total_loss: 0.419 (x1.0)\n","INFO:Train145:Train Epoch: [1][4/100] Time: 320.643 lr: 0.001000 l1_loss: 0.1684 (x1.0) ssim_loss: 0.2506 (x1.0) total_loss: 0.419 (x1.0)\n","2024-12-11 17:34:24,656 - Train Epoch: [1][5/100] Time: 429.353 lr: 0.001000 l1_loss: 0.1342 (x1.0) ssim_loss: 0.2268 (x1.0) total_loss: 0.361 (x1.0)\n","INFO:Train145:Train Epoch: [1][5/100] Time: 429.353 lr: 0.001000 l1_loss: 0.1342 (x1.0) ssim_loss: 0.2268 (x1.0) total_loss: 0.361 (x1.0)\n","2024-12-11 17:36:09,373 - Train Epoch: [1][6/100] Time: 532.209 lr: 0.001000 l1_loss: 0.1449 (x1.0) ssim_loss: 0.2056 (x1.0) total_loss: 0.3505 (x1.0)\n","INFO:Train145:Train Epoch: [1][6/100] Time: 532.209 lr: 0.001000 l1_loss: 0.1449 (x1.0) ssim_loss: 0.2056 (x1.0) total_loss: 0.3505 (x1.0)\n","2024-12-11 17:37:52,846 - Train Epoch: [1][7/100] Time: 636.844 lr: 0.001000 l1_loss: 0.1339 (x1.0) ssim_loss: 0.1991 (x1.0) total_loss: 0.333 (x1.0)\n","INFO:Train145:Train Epoch: [1][7/100] Time: 636.844 lr: 0.001000 l1_loss: 0.1339 (x1.0) ssim_loss: 0.1991 (x1.0) total_loss: 0.333 (x1.0)\n","2024-12-11 17:39:34,727 - Train Epoch: [1][8/100] Time: 740.358 lr: 0.001000 l1_loss: 0.1352 (x1.0) ssim_loss: 0.2198 (x1.0) total_loss: 0.3551 (x1.0)\n","INFO:Train145:Train Epoch: [1][8/100] Time: 740.358 lr: 0.001000 l1_loss: 0.1352 (x1.0) ssim_loss: 0.2198 (x1.0) total_loss: 0.3551 (x1.0)\n","2024-12-11 17:41:16,479 - Train Epoch: [1][9/100] Time: 841.975 lr: 0.001000 l1_loss: 0.1438 (x1.0) ssim_loss: 0.1969 (x1.0) total_loss: 0.3407 (x1.0)\n","INFO:Train145:Train Epoch: [1][9/100] Time: 841.975 lr: 0.001000 l1_loss: 0.1438 (x1.0) ssim_loss: 0.1969 (x1.0) total_loss: 0.3407 (x1.0)\n","2024-12-11 17:43:00,505 - Train Epoch: [1][10/100] Time: 944.167 lr: 0.001000 l1_loss: 0.1417 (x1.0) ssim_loss: 0.1844 (x1.0) total_loss: 0.3261 (x1.0)\n","INFO:Train145:Train Epoch: [1][10/100] Time: 944.167 lr: 0.001000 l1_loss: 0.1417 (x1.0) ssim_loss: 0.1844 (x1.0) total_loss: 0.3261 (x1.0)\n","2024-12-11 17:44:41,603 - Train Epoch: [1][11/100] Time: 1047.946 lr: 0.001000 l1_loss: 0.1088 (x1.0) ssim_loss: 0.1669 (x1.0) total_loss: 0.2757 (x1.0)\n","INFO:Train145:Train Epoch: [1][11/100] Time: 1047.946 lr: 0.001000 l1_loss: 0.1088 (x1.0) ssim_loss: 0.1669 (x1.0) total_loss: 0.2757 (x1.0)\n","2024-12-11 17:46:23,837 - Train Epoch: [1][12/100] Time: 1149.111 lr: 0.001000 l1_loss: 0.1047 (x1.0) ssim_loss: 0.1623 (x1.0) total_loss: 0.267 (x1.0)\n","INFO:Train145:Train Epoch: [1][12/100] Time: 1149.111 lr: 0.001000 l1_loss: 0.1047 (x1.0) ssim_loss: 0.1623 (x1.0) total_loss: 0.267 (x1.0)\n","2024-12-11 17:48:07,432 - Train Epoch: [1][13/100] Time: 1251.221 lr: 0.001000 l1_loss: 0.1021 (x1.0) ssim_loss: 0.176 (x1.0) total_loss: 0.2781 (x1.0)\n","INFO:Train145:Train Epoch: [1][13/100] Time: 1251.221 lr: 0.001000 l1_loss: 0.1021 (x1.0) ssim_loss: 0.176 (x1.0) total_loss: 0.2781 (x1.0)\n","2024-12-11 17:49:48,908 - Train Epoch: [1][14/100] Time: 1354.772 lr: 0.001000 l1_loss: 0.1076 (x1.0) ssim_loss: 0.1498 (x1.0) total_loss: 0.2574 (x1.0)\n","INFO:Train145:Train Epoch: [1][14/100] Time: 1354.772 lr: 0.001000 l1_loss: 0.1076 (x1.0) ssim_loss: 0.1498 (x1.0) total_loss: 0.2574 (x1.0)\n"]}]}]}