{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/UIE-Lab/HUWIE-Net/blob/main/train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ek6OyRc6sexa"
      },
      "source": [
        "\n",
        "# **Training HUWIE-Net**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wB6uavxPmiMh"
      },
      "source": [
        "**Cloning the HUWIE-Net Repository**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N1Zb9cxYJl-a"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/UIE-Lab/HUWIE-Net.git /content/Experiments/HUWIE-Net"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JfM7EPkzm1g3"
      },
      "source": [
        "**Downloading and Extracting the Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nTaYUjlPKEZq"
      },
      "outputs": [],
      "source": [
        "!pip install gdown\n",
        "!gdown --id 1ft9mlhO5JodH3Aw9Ak_l3tMz8pjNec9p\n",
        "!unzip Data.zip -d ./\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mi4wCqoEnZFl"
      },
      "source": [
        "**Importing Modules**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U42sF-SVKtXg"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.chdir('/content/Experiments/HUWIE-Net')\n",
        "\n",
        "import sys\n",
        "import time\n",
        "import argparse\n",
        "from getpass import getuser\n",
        "from socket import gethostname\n",
        "import logging\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from core.models import HUWIE_Net, HUWIE_Net_I2IM, HUWIE_Net_PIM\n",
        "from core.losses import Loss\n",
        "from core.datasets import UIEBD\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pjfbqr1hnrSa"
      },
      "source": [
        "**Configuration and Setup**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I-X9QG7VLbWT"
      },
      "outputs": [],
      "source": [
        "parser = argparse.ArgumentParser(description='OD')\n",
        "parser.add_argument('--name', type=str, default='Train145')\n",
        "parser.add_argument('--work_dir', type=str, default='../../Data/checkpoints/')\n",
        "parser.add_argument('--UIEBD_konf', type=int, default=3)\n",
        "parser.add_argument('--model', type=object, default=HUWIE_Net)\n",
        "parser.add_argument('--loss', type=object, default=Loss)\n",
        "parser.add_argument('--epochs', type=int, default=1)\n",
        "parser.add_argument('--lr', type=float, default=1e-3)\n",
        "parser.add_argument('--train_val_batch_size', type=int, default=8)\n",
        "parser.add_argument('--test_batch_size', type=int, default=1)\n",
        "parser.add_argument('--step_size', type=int, default=50)\n",
        "parser.add_argument('--gamma', type=float, default=0.5)\n",
        "args, unknown = parser.parse_known_args()\n",
        "print(args)\n",
        "\n",
        "timestamp = time.strftime('%Y%m%d_%H%M%S', time.localtime())\n",
        "work_dir = args.work_dir + args.name + '_' + args.model.__name__ + '_' + timestamp + '/'\n",
        "p = os.path.abspath(work_dir)\n",
        "if not os.path.exists(p): os.makedirs(p)\n",
        "work_dir_img_output = work_dir + 'img_output' '/'\n",
        "p = os.path.abspath(work_dir_img_output)\n",
        "if not os.path.exists(work_dir_img_output): os.makedirs(work_dir_img_output)\n",
        "\n",
        "# create text log\n",
        "logger = logging.getLogger(args.name)\n",
        "log_file = os.path.join(work_dir, f'{timestamp}.log')\n",
        "formatter = logging.Formatter('%(asctime)s - %(message)s')\n",
        "stream_handler = logging.StreamHandler()\n",
        "stream_handler.setFormatter(formatter)\n",
        "logger.addHandler(stream_handler)\n",
        "file_handler = logging.FileHandler(log_file, 'w', encoding='utf-8')\n",
        "file_handler.setFormatter(formatter)\n",
        "logger.addHandler(file_handler)\n",
        "logger.setLevel(logging.INFO)\n",
        "logger.info(args)\n",
        "\n",
        "# tensorboard log\n",
        "writer = SummaryWriter(os.path.join(work_dir, 'tensorboard_logs'))\n",
        "\n",
        "# dataset\n",
        "konf = args.UIEBD_konf\n",
        "train_dataset = UIEBD(data_type='train', konf=konf)\n",
        "val_dataset = UIEBD(data_type='val', konf=konf)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=args.train_val_batch_size, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=args.train_val_batch_size, shuffle=True)\n",
        "logger.info('Dataset: ' + train_dataset.__class__.__name__)\n",
        "\n",
        "# model\n",
        "model = args.model()\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "logger.info('Device: ' + str(device))\n",
        "model.to(device)\n",
        "\n",
        "# print model's state_dict\n",
        "logger.info(\"Model's state_dict:\")\n",
        "for param_tensor in model.state_dict():\n",
        "    logger.info(param_tensor + \" - \" + str(model.state_dict()[param_tensor].size()))\n",
        "logger.info('Finish Build Model')\n",
        "\n",
        "# optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=args.step_size, gamma=args.gamma)\n",
        "\n",
        "# print optimizer's state_dict\n",
        "logger.info(\"Optimizer's state_dict:\")\n",
        "for var_name in optimizer.state_dict():\n",
        "    logger.info(var_name + \" - \" + str(optimizer.state_dict()[var_name]))\n",
        "logger.info('Finish Build Optimizer')\n",
        "\n",
        "# loss\n",
        "criterion = args.loss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUOTipJ3nzTt"
      },
      "source": [
        "**Training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "V1kNLWwvM61U"
      },
      "outputs": [],
      "source": [
        "# start\n",
        "train_iter = args.epochs * len(train_dataloader)\n",
        "val_iter = args.epochs * len(val_dataloader)\n",
        "\n",
        "logger.info('Host: %s, Work Dir: %s', f'{getuser()}@{gethostname()}', work_dir)\n",
        "logger.info('Epoch: %d, Train Iteration: %d, Validation Iteration: %d', args.epochs, train_iter, val_iter)\n",
        "\n",
        "logger.info('Start')\n",
        "t = time.time()\n",
        "\n",
        "# Çalışan kodda yüklenen tüm kütüphaneleri al\n",
        "loaded_modules = list(sys.modules.keys())\n",
        "with open(\"requirements.txt\", \"w\") as file:\n",
        "    for module in loaded_modules:\n",
        "        file.write(module + \"\\n\")\n",
        "\n",
        "for epoch in range(1, args.epochs + 1):\n",
        "\n",
        "    logger.info('Epoch %d', epoch)\n",
        "\n",
        "    ###---TRAIN---###\n",
        "\n",
        "    train_epoch_loss_item = np.zeros(criterion.loss_fn_num, dtype='float32')\n",
        "\n",
        "    model.train()\n",
        "    for i, tdata in enumerate(train_dataloader):\n",
        "\n",
        "        data_time = time.time() - t\n",
        "\n",
        "        inputs = tdata['raw_data'].to(device)\n",
        "        labels = tdata['gt_data'].to(device)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        losses, weight = criterion(outputs, labels)\n",
        "        total_loss = losses[-1]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        total_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        logger.info('Train Epoch: [%d][%d/%d] Time: %.3f lr: %f ' +\n",
        "                    ' '.join([criterion.loss_name[k] + ': ' + str(np.round(losses[k].item(), 4)) + ' (x' + str(weight[k]) + ')' for k in range(len(losses))]),\n",
        "                    epoch, i+1, len(train_dataloader), data_time, optimizer.param_groups[0]['lr'])\n",
        "\n",
        "        for k in range(len(losses)):\n",
        "            train_epoch_loss_item[k] += losses[k].item()\n",
        "\n",
        "        for loss in losses:\n",
        "            if torch.isnan(loss):\n",
        "                torch.save(model.state_dict(), work_dir + 'nan_loss_' + 'epoch{}.pth'.format(epoch))\n",
        "                logger.info('NaN loss...')\n",
        "                sys.exit()\n",
        "\n",
        "    train_epoch_loss_item = train_epoch_loss_item / len(train_dataloader)\n",
        "\n",
        "    logger.info('Train Epoch (Average): [%d] ' + ' '.join([criterion.loss_name[k] + ': ' + str(np.round(train_epoch_loss_item[k], 4)) for k in range(len(losses))]), epoch)\n",
        "\n",
        "    for k in range(len(train_epoch_loss_item)):\n",
        "        writer.add_scalars('Train vs. Val Loss', {'Train_' + criterion.loss_name[k]: train_epoch_loss_item[k]}, epoch)\n",
        "\n",
        "    for name, param in model.named_parameters():\n",
        "        writer.add_histogram('model_param/' + name, param, epoch)\n",
        "        writer.add_histogram('model_param_grad/' + name, param.grad, epoch)\n",
        "        writer.add_scalar('model_param_grad_abs_sum/' + name, torch.sum(torch.abs(param.grad)), epoch)\n",
        "\n",
        "    if epoch == args.epochs:\n",
        "        torch.save(model.state_dict(), work_dir + args.model.__name__ + '_epoch{}.pth'.format(epoch))\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    ###---VALIDATION---###\n",
        "\n",
        "    val_epoch_loss_item = np.zeros(criterion.loss_fn_num, dtype='float32')\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for j, vdata in enumerate(val_dataloader):\n",
        "\n",
        "            data_time = time.time() - t\n",
        "\n",
        "            vinputs = tdata['raw_data'].to(device)\n",
        "            vlabels = tdata['gt_data'].to(device)\n",
        "\n",
        "            voutputs  = model(vinputs)\n",
        "\n",
        "            losses, weight = criterion(voutputs, vlabels)\n",
        "            total_loss = losses[-1]\n",
        "\n",
        "            logger.info('Val Epoch: [%d][%d/%d] Time: %.3f ' +\n",
        "                        ' '.join([criterion.loss_name[k] + ': ' + str(np.round(losses[k].item(), 4)) + ' (x' + str(weight[k]) + ')' for k in range(len(losses))]),\n",
        "                        epoch, i+1, len(val_dataloader), data_time)\n",
        "\n",
        "            for k in range(len(losses)):\n",
        "                val_epoch_loss_item[k] += losses[k].item()\n",
        "\n",
        "    val_epoch_loss_item = val_epoch_loss_item / len(val_dataloader)\n",
        "\n",
        "    logger.info('Val Epoch (Average): [%d] ' + ' '.join([criterion.loss_name[k] + ': ' + str(np.round(val_epoch_loss_item[k], 4)) for k in range(len(losses))]), epoch)\n",
        "\n",
        "    for k in range(len(train_epoch_loss_item)):\n",
        "        writer.add_scalars('Train vs. Val Loss', {'Val_' + criterion.loss_name[k]: val_epoch_loss_item[k]}, epoch)\n",
        "\n",
        "writer.flush()\n",
        "writer.close()\n",
        "\n",
        "logger.info('Finish')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}